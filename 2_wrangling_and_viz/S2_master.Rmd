---
title: "Session 2: Data Wrangling and Visualization in R"
output: html_document
---

# Agenda

- [Introducing the Tidyverse](#introducing-the-tidyverse): standardized and idiomatic data analysis; basic commands. ([Exercises](#tidyverse-exercises))
- [Introducing the Grammar of Graphics](#introducing-the-grammar-of-graphics): what is a grammar of graphics and why do I care?; basic plots. ([Exercises](#ggplot-exercises))
- [Going Wider and Deeper](#going-wider-and-deeper): understanding the tidy philosophy; enhancing visualization; multiple datasets

# Introducing the Tidyverse: `dplyr`

[(back to top)](#agenda)

Base R may be slow and arcane, but we'll be going over a set of packages that make R a joy to work with for data wrangling. Collectively known as the Tidyverse, these packages are more or less standard for any practitioner working with R. They provide a clean and idomatic language for manipulating and visualizing data.


## Loading the libraries


First we need to load the packages.  If you did the homework, you already have them installed, but if not (shame!) install them with: `install.packages('tidyr')` and `install.packages('dplyr')`.

Okay, now we'll load them into our current R session by calling:

```{r message=FALSE}
library(tidyr)
library(dplyr)
```

## Basic exploration and cleaning

Let's load up the AirBnB data.  Remember to *set the working directory* to the folder with the data in it (one easy to do this is in the Files tab using the "More" drop-down menu).  Then, in a fresh script (or following along in the class script), type and execute:

```{r, echo=FALSE}
raw_listings <- read.csv('../data/listings.csv')
```


Let's do some basic exploration on the data. How about `select`ing a specific column, and looking at the first few rows:
```{r}
head(select(raw_listings, price))
```

This is fine, but it's a little awkward having to nest our code like that.  Luckily, there is a nifty operator included with tidyr called the **chaining operator** which looks like `%>%` and serves like a pipeline from one function to another. Specifically: `x %>% f == f(x)`. Now we can instead do this:

```{r}
raw_listings %>% select(price) %>% head()
```

which is much, much nicer.  Notice that the chaining operator feeds in the object on its left as the first argument into the function on its right.

You might notice that these prices are actually strings. If we want to work with these as numbers, we'll need to convert them. Let's try using `mutate`, which allows us to create new columns -- often by referring to existing ones.

```{r}
raw_listings %>%
  mutate(nprice = as.numeric(gsub('\\$|,', '', price))) %>%
  select(name, price, nprice)
```

Note that the tidyverse packages generally do not change the dataframe objects they act on. For example, the code above doesn't change `listings`, but instead returns a new dataframe that has the same data as `listings`, plus an extra column.

Now, let's learn some more verbs. Let's say we're interested in understanding the relationship between bedrooms and price. But some of the listings don't have data on bedrooms; can we `count` how many?

```{r}
raw_listings %>% count(is.na(bedrooms))
```

Let's filter these out:

```{r}
raw_listings %>% filter(!is.na(bedrooms))
```

Finally, let's combine all of these to make a clean dataset to work with. We want to make sure our data has a correct price column, no missing bedroom column, and only has a subset of the ~100 columns we started out with. We'll assign it to a dataframe named `listings`.


```{r}
listings <- raw_listings %>%
  filter(!is.na(bedrooms)) %>%
  mutate(price = as.numeric(gsub('\\$|,', '', price))) %>%
  select(name, price, bedrooms,
         neighbourhood_cleansed, latitude, longitude, review_scores_rating)
```


## Aggregation
Now, how about a summary statistic, like the average price for a listing? Let's take our clean dataset and `summarize` it to find out.


Hmm.  Does it have anything to do with just recent listings?  Let's do a table to `summarize` the number of reviews for an NA entry by showing the average number of reviews:
```{r}
listings %>%
  filter(is.na(reviews_per_month)) %>%
  summarize(avg.num.reviews = mean(number_of_reviews))
```

Ah, so these are just listings without any reviews yet.  That's not alarming.  (**Note to international students:** `summarise` also works!)


Let's take a deeper look at prices, and we can make our lives easier by just overwriting that price column with the numeric version and saving it back into our `listings` data frame:
```{r}
listings <- listings %>% mutate(price = as.numeric(gsub('\\$|,', '', price)))
```

Now --- what if we want to look at mean price, and `group_by` neighborhood?
```{r}
listings %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(avg.price = mean(price))
```

Maybe we're a little worried these averages are skewed by a few outlier listings. Let's try
```{r}
listings %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(avg.price = mean(price),
            med.price = median(price),
            num = n())
```

The `n()` function here just gives a count of how many rows we have in each group. Nothing too crazy, but we do notice some red flags to our "mean" approach.

- First, if there are a very small number of listings in a neighborhood compared to the rest of the dataset, we may worry we don't have a representative sample, or that this data point should be discredited somehow (on the other hand, maybe it's just a small neighborhood, like Bay Village, and it's actually outperforming expectation).

- Second, if the *median* is very different than the *mean* for a particular neighborhood, it indicates that we have *outliers* skewing the average.  Because of those outliers, as a rule of thumb, means tend to be a misleading statistic to use with things like rent prices or incomes.

One thing we can do is just filter out any neighborhood below a threshold count:
```{r}
listings %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(avg.price = mean(price),
            med.price = median(price),
            num = n()) %>%
  filter(num > 200)
```

We can also `arrange` this info (sort it) by the hopefully more meaningful median price:
```{r}
listings %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(avg.price = mean(price),
            med.price = median(price),
            num = n()) %>%
  filter(num > 200) %>%
  arrange(med.price)
```

(Descending order would just be `arrange(desc(med.price))`.)  We can also pick a few neighborhoods to look at by using the `%in%` keyword in a `filter` command with a list of the neighborhoods we want:
```{r}
listings %>%
  filter(neighbourhood_cleansed %in% c('Downtown', 'Back Bay', 'Chinatown')) %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(avg.price = mean(price),
            med.price = median(price),
            num = n()) %>%
  arrange(med.price)
```

We have now seen: `select`, `filter`, `count`, `summarize`, `mutate`, `group_by`, and `arrange`.  This is the majority of the dplyr "verbs" for operating on a single data table (although [there are many more](https://cran.r-project.org/web/packages/dplyr/dplyr.pdf)), but as you can see, learning new verbs is pretty intuitive. What we have already gives us enough tools to accomplish a large swath of data analysis tasks.

But ... we'd really like to visualize some of this data, not just scan summary tables.  Next up, `ggplot`.




# Tidyverse Exercises

[(back to top)](#agenda)

We'll now introduce a few new tricks for some of the dplyr verbs we covered earlier, but this is by no means a comprehensive treatment.

**Exercise 1. More with `select`.**  In addition to selecting columns, `select` is useful for temporarily renaming columns.  We simply do an assignment, for example `select('New colname'=old_col_name)`.  This is helpful for display purposes when our column names are hideous.  Try generating the summary table of median price by room type but assigning some nicer column labels.

*ANSWER:*
```{r}
listings %>%
  mutate(price = as.numeric(gsub('\\$|,','',price))) %>%
  group_by(room_type) %>%
  summarize(med = median(price)) %>%
  select('Room type'=room_type, 'Median price'=med)
```

Another useful trick with select (and other functions in R) is to include *all but* a column by using the minus `-` sign before the excluded column.  For example `listings %>% select(-id)` selects every column *except* the listing ID.


**Exercise 2. More with `group_by`.**  We can group by multiple columns, and dplyr will start cross-tabulating the information within each group.  For example, let's say we want the count of listings by room type and accommodation, we could do
```{r}
listings %>% group_by(room_type, accommodates) %>% count()
```

This is the same information we got earlier using a `table` command (although in an interestingly *longer* format, which we will talk about later).  Try finding the median daily price of a listing, grouped by number of bedrooms and number of bathrooms:

*ANSWER:*
```{r}
listings %>%
  mutate(price = as.numeric(gsub('\\$|,','',price))) %>%
  group_by(bedrooms, bathrooms) %>%
  summarize(med = median(price))
```


**Exercise 3. More with `mutate`.**  The code block earlier with multiple mutation commands got a little repetitive, and we are lazy.  We would rather have a verb so we can select some columns, and apply some function to `mutate_all` of them:
```{r}
listings %>%
  select(price, weekly_price, monthly_price) %>%
  mutate_all(funs(numversion = as.numeric(gsub('\\$|,', '', .)))) %>%
  head()
```

This is fairly straightforward, with two "tricks": `funs()` is a convenience function we have to use to tell dplyr to apply the transformation to multiple columns, and the period `.` serves as a stand-in for the column we're on.  Note also we have created new columns which tack on "_numversion" to the older columns, but if we leave out that assignment in `funs()` we just overwrite the previous columns.  If we want to be able to specify which columns we want to `mutate_at`, we can do:
```{r}
listings %>%
  select(name, price, weekly_price, monthly_price) %>%
  mutate_at(c('price', 'weekly_price', 'monthly_price'),  # specify a list of cols
            funs(as.numeric(gsub('\\$|,', '', .)))) %>%   # specify the transformation
  head()
```

This time also notice that we actually didn't make new columns, we mutated the existing ones.

(There is also a variation for conditional operations (`mutate_if`) and analogous versions of all of this for summarize (`summarize_all`, ...).  We don't have time to cover them all, but if you ever need it, you know it's out there!)

Try using one of these methods to convert all the date columns to `Date` (fortunately they all use the same formatting).

*ANSWER:*
```{r}
listings %>%
  select(last_scraped, host_since, first_review, last_review) %>%
  mutate_all(funs(as.Date(., "%Y-%m-%d"))) %>%
  head()
```



# Introducing the Grammar of Graphics

[(back to top)](#agenda)

`ggplot` provides a unifying approach to graphics, similar to what we've begun to see with tidyr. ggplot was created by Leland Wilkinson with his book [The Grammar of Graphics](https://www.cs.uic.edu/~wilkinson/TheGrammarOfGraphics/GOG.html) (which is the gg in ggplot), and put into code by Hadley Wickham.  We'll see it not only provides a clean way of approaching data visualization, but also nests with the tidyr universe like a hand in a glove.

## Philosophy

What does **grammar of graphics** mean?  A grammar is a set of guidelines for how to combine components (ingredients) to create new things.  One example is the grammar of language: in English, you can combine a noun (like "the dog") and a verb (like "runs") to create a sentence ("the dog runs").

Let's translate this idea to visualization. Every ggplot consists of three main elements:

- **Data**: The dataframe we want to plot.
- **Aes**thetics: The dimensions we want to plot, e.g. x, y, color, size, shape, etc.
- **Geom**etry:  The specific visualization shape. Line plot, scatter plot, bar plot, etc.

A couple less important parts:
- **Stat**istical transformation.  How should the data be transformed or aggregated before visualizing?
- **Theme**.  This is like flavoring: how do we want the chart to look and feel?

## Example

First, make sure you've got `ggplot2` installed (with `install.packages('ggplot2')`) and then load it into your session:

```{r message=FALSE}
library(ggplot2)
```

Now, we chain this into the `ggplot` function...
```{r}
listings %>%
  ggplot(aes(x=review_scores_rating, y=price)) +
  geom_point()
```

Behold: we specify our Data (`listings`), our Aesthetic mapping (`x` and `y` to columns of the data), and our desired Geometry (`geom_point`).  We are gluing each new element together with `+` signs.   Clean, intuitive, and already a little prettier than the Base R version.  But most importantly, this is much more extensible.  Let's see how.

First, let's try grouping listings together if they have the same review score, and take the median within the group, and plot that.  (This is a little weird since the score could take continuous values and we should be binning them... but let's see what happens.)  Oh, and just filter out those pesky NAs.

```{r}
listings %>%
  filter(!is.na(review_scores_rating)) %>%
  group_by(review_scores_rating) %>%
  summarize(med.price = median(price)) %>%
  ggplot(aes(x=review_scores_rating, y=med.price)) +
  geom_point()
```

Now that is a bit interesting, and definitely easy to see a trend now.

### Cleaning it up

Let's clean up this chart by coloring the points blue and customizing the axis labels:

```{r}
listings %>%
  filter(!is.na(review_scores_rating)) %>%
  group_by(review_scores_rating) %>%
  summarize(med.price = median(price)) %>%
  ggplot(aes(x=review_scores_rating, y=med.price)) +
  geom_point(color='blue') +
  labs(x='Score', y='Median Price', title='Listing Price Trend by Review Score')
```

That's a little better.  (**Note to international students:** `colour` also works!)  Maybe we are worried that some of those dots represent only a few listings, and we want to visually see where the center of mass is for this trend.  Let's add back in that `n()` count from before to our summarize function, and add in an additional aesthetic mapping to govern the *size* of our geometry:

```{r}
listings %>%
  filter(!is.na(review_scores_rating)) %>%
  group_by(review_scores_rating) %>%
  summarize(med.price = median(price),
            num = n()) %>%
  ggplot(aes(x=review_scores_rating, y=med.price, size=num)) +
  geom_point(color='blue') +
  labs(x='Score', y='Median Price', title='Median Price Trend by Review Score')
```

Those blue dots got a little crowded.  Let's put in some transparency by adjusting the alpha level in our geometry, and change the background to white by changing our theme.  Oh, and let's relabel that legend (notice we specify the labels for each aesthetic mapping, so `size=` will set the legend title, since the legend shows the size mapping).

```{r}
listings %>%
  filter(!is.na(review_scores_rating)) %>%
  group_by(review_scores_rating) %>%
  summarize(med.price = median(price),
            num = n()) %>%
  ggplot(aes(x=review_scores_rating, y=med.price, size=num)) +
  geom_point(color='blue', alpha=0.5) +
  labs(x='Score', y='Median Price', size='# Reviews',
       title='Median Price Trend by Review Score') +
  theme_bw()
```

That looks pretty good if you ask me!  And, like any decent visualization, it tells a story, and raises interesting questions: there appears to be a correlation between the score of a listing and the price --- is this because higher rated listings can ask a higher price on average? or because when you pay top dollar you trick yourself into believing it was a nicer stay?  What is the best predictor of price, or ratings, or other variables?  We'll explore some of these questions in the next session.


### Saving a plot

By the way, you can flip back through all the plots you've created in RStudio using the navigation arrows, and it's also always a good idea to "Zoom" in on plots.

Also, when you finally get one you like, you can "Export" it to a PDF (recommended), image, or just to the clipboard.  Another way to save a plot is to use `ggsave()`, which saves the last plot by default, for example: `ggsave('price_vs_score.pdf')`.


## Other geometries: Line plots, Box plots, and Bars

We will now quickly run through a few of the other geometry options available, but truly, we don't need to spend a lot of time here since each follows the same grammar as before --- the beauty of ggplot! Note that geoms are stackable -- we can just keep adding them on top of each other.

For example, let's look at the price by neighborhood again.  First let's save the summary information we want to plot into its own object:

```{r}
by.neighbor <- listings %>%
  group_by(neighbourhood_cleansed) %>%
  summarize(med.price = median(price))
```

We've already seen `geom_point`, now let's try now adding on `geom_line`:

```{r}
by.neighbor %>%
  ggplot(aes(x=neighbourhood_cleansed, y=med.price)) +
  geom_point() +
  geom_line(group=1)
```

This is misleading, since it falsely implies continuity of price between neighborhoods based on the arbitrary alphabetical ordering.  Also, because our `x` is not a continuous variable, but a list of neighborhoods, `geom_line` thinks the neighborhoods are categories that each need their own line --- so we had to specify `group=1` to group everything into one line.

So we've seen a line, but its not appropriate for our visualization purpose here.  Let's switch to a bar chart, i.e. `geom_bar`.  Oh, and let's rotate the labels on the x-axis so we can read them:

```{r}
by.neighbor %>%
  ggplot(aes(x=neighbourhood_cleansed, y=med.price)) +
  geom_bar(stat='identity') +
  theme(axis.text.x=element_text(angle=60, hjust=1))
```

Again, notice we are separating thematic (non-content) adjustments like text rotation, from geometry, from aesthetic mappings.  (Try playing around with the settings!)

Also notice we added an argument to `geom_bar`: `stat='identity'`.  This tells `geom_bar` that we want the height of the bar to be equal to the `y` value (identity in math means "same as" or "multiplied by one").  We could have instead told it to set the height of the bar based on an aggregate count of different x values, or by binning similar values together --- we'll cover this idea of binning more in the next subsection.

For now, let's follow-through on this idea and clean up this plot a bit:

```{r}
by.neighbor %>%
  ggplot(aes(x=reorder(neighbourhood_cleansed, -med.price), y=med.price)) +
  geom_bar(fill='dark blue', stat='identity') +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  labs(x='', y='Median price', title='Median daily price by neighborhood')
```

Only new tool here is the `reorder` function we used in the `x` aesthetic, which simply reorders the first argument in order by the last argument, which in our case was the (negative) median price (so we get a descending order).

Again we have an interesting visualization because it raises a couple questions:

- What explains the steep dropoff from "North End" to "Jamaica Plain"?  (Is this a central-Boston vs. peripheral-Boston effect? What would be some good ways to visualize that?)

- Does this ordering vary over time, or is the Leather District always the most high end?

- What is the distribution of prices in some of these neighborhoods? -- we have tried to take care of outliers by using the median, but we still have a hard time getting a feel for a neighborhood with just a single number.

Toward the first two observations/questions, we'll see how to incorporate maps into our visualizations in Session 2 and 3, and we'll see some ways to approach "time series" questions in Session 3.

For now, let's pick out a few of these high end neighborhoods and plot a more detailed view of the distribution of price using `geom_boxplot`.  We also need to pipe in the full dataset now so we have the full price information, not just the summary info.
```{r}
listings %>%
  filter(neighbourhood_cleansed %in% c('South Boston Waterfront', 'Bay Village',
                                       'Leather District', 'Back Bay', 'Downtown')) %>%
  ggplot(aes(x=neighbourhood_cleansed, y=price)) +
  geom_boxplot()
```

A boxplot shows the 25th and 75th percentiles (top and bottom of the box), the 50th percentile or median (thick middle line), the max/min values (top/bottom vertical lines), and outliers (dots).  By simply changing our geometry command, we now see that although the medians were very similar, the distributions were quite different (with Bay Village especially having a "heavy tail" of expensive properties), and there are many extreme outliers ($3000 *a night*?!).


### Histograms, Palettes, and Multiple plots

Near the beginning of this session we did a histogram of the price in base R, which shows the count of the number of values that occur within sequential intervals (the intervals are called "bins" and the process of counting how many occurrences belong in each interval is called "binning").  We can easily do this with ggplot using `geom_histogram`.  Let's try bin widths of 50 (dollars):

```{r}
listings %>%
  ggplot(aes(x=price)) +
  geom_histogram(binwidth=50)
```

This is okay but let's trim off those outliers, clean up the colors, and add labels.  We also want to make sure each bar is `centered` in the middle of the interval, which we expect to be 0-50, 51-100, etc.  We can do this by setting the `center=` argument in `geom_histogram`, like so:

```{r}
listings %>%
  filter(price < 500) %>%
  ggplot(aes(x=price)) +
  geom_histogram(binwidth=50, center=25, fill='cyan', color='black') +
  labs(x='Price', y='# Listings')
```

(Reality-check it with `listings %>% filter(price <= 50) %>% count()`.)  Another way to represent this information is using lines (instead of bars):

```{r}
listings %>%
  filter(price < 500) %>%
  ggplot(aes(x=price)) +
  geom_freqpoly(binwidth=50, color='black') +
  labs(x='Price', y='# Listings')
```

Let's say we want to compare this distribution of price for different room types.  We can set the fill color of the histogram to map to the room type:

```{r}
listings %>%
  filter(price < 500) %>%
  ggplot(aes(x=price, fill=room_type)) +
  geom_histogram(binwidth=50, center=25) +
  labs(x='Price', y='# Listings', fill='Room type')
```

Hmmm, this is a little hard to interpret because there are very different numbers of listings for the different types of room. It's also stacked, not overlaid.

We can overlay the bars by setting the `position=` argument to `identity` (i.e. don't change the position of any bars), although we should then also add in a little transparency.  We would also like to "normalize" the values --- divide each bar height by the total sum of all the bars --- so that each bar represents the fraction of listings with that price range, instead of the count.  We can do that with a special mapping in `aes` to make `y` a "density", like so:

```{r}
listings %>%
  filter(price < 500) %>%
  ggplot(aes(x=price, y=..density.., fill=room_type)) +
  geom_histogram(binwidth=50, center=25, position='identity', alpha=0.5, color='black') +
  labs(x='Price', y='Frac. of Listings', fill='Room type')
```

Maybe we'd prefer to have the histogram bars adjacent, not stacked, so we'll tell `geom_histogram` to set each bar's position by `dodge`ing the others:

```{r}
listings %>%
  filter(price < 500) %>%
  ggplot(aes(x=price, y=..density.., fill=room_type)) +
  geom_histogram(binwidth=50, center=25, position='dodge', color='black') +
  labs(x='Price', y='Frac. of Listings', fill='Room type')
```

Now I'm not crazy about any of this since the default color scheme (or **palette**) involves green and red, and I am red-green colorblind.  Woe is me and my fellow colorblind peoples.  But this is just a matter of specifying our palette, and fortunately there is a colorblind-friendly palette which we can load into our session as a list of hexadecimal color codes:

```{r}
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73",
               "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

(There are dozens of palettes, find more [here](http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/).)  Now we can tack on the thematic command `scale_fill_manual` which will `manual`ly set the `values` of our `fill` from this palette (i.e. list of colors), and we get:
```{r}
listings %>%
  filter(price < 500) %>%
  ggplot(aes(x=price, y=..density.., fill=room_type)) +
  geom_histogram(binwidth=50, center=25, position='dodge', color='black') +
  labs(x='Price', y='Frac. of Listings', fill='Room type') +
  scale_fill_manual(values=cbbPalette)
```

Ahhhh.  There are many other ways of setting colors (for fill or lines) this way, even without specifying a custom palette: gradient valued, continuous, etc.  You can check them out in the help by typing `?scale_fill_` or `?scale_color_` and selecting from the inline menu.

This is still a little hard to read because the bars are right next to each other ... maybe we could try `geom_freqpoly` again?

```{r}
listings %>%
  filter(price < 500) %>%
  ggplot(aes(x=price, y=..density.., color=room_type)) +
  geom_freqpoly(binwidth=50) +
  labs(x='Price', y='Frac. of Listings', color='Room type') +
  scale_color_manual(values=cbbPalette)
```

Compare the changes we made to the aesthetic mapping and palette command from the previous plot to this one --- we are mapping the palette to line color, not fill, so we had to make a few tweaks.

Importantly, it is now easy to see that "shared room" listings run cheaper more often than other room types, and almost all the pricey rooms are "entire home/apt".  Nothing surprising here, but good to be able to check our intuitions in the actual data.

## Statistical fanciness

The only major feature of ggplot we have not yet covered is the *statistical transformation* ability.  This allows you to slap a statistical treatment on a base visualization --- for example, a linear regression over a scatter plot, or a normal distribution over a histogram.  We will show a few examples of this feature, but it is an extremely powerful addition and we leave deeper treatment for future sessions.

Let's fit a trend line to that scatter plot of average price vs review score by adding a `geom_smooth` call.

```{r}
listings %>%
  filter(!is.na(review_scores_rating)) %>%
  group_by(review_scores_rating) %>%
  summarize(med.price = median(price),
            num = n()) %>%
  ggplot(aes(x=review_scores_rating, y=med.price, size=num)) +
  geom_point(color='blue', alpha=0.5) +
  labs(x='Score', y='Median Price', size='# Reviews',
       title='Median Price Trend by Review Score') +
  geom_smooth()
```

Note that all we did was take our previous plot commands, and added the `geom_smooth` command.  The default is to use a "loess" fit (locally weighted scatterplot smoothing), which is a bit overcomplicated for our data.  We can instead specify the model, for example to just fit a line (linear regression) we say

```{r}
listings %>%
  filter(!is.na(review_scores_rating)) %>%
  group_by(review_scores_rating) %>%
  summarize(med.price = median(price),
            num = n()) %>%
  ggplot(aes(x=review_scores_rating, y=med.price, size=num)) +
  geom_point(color='blue', alpha=0.5) +
  labs(x='Score', y='Median Price', size='# Reviews',
       title='Median Price Trend by Review Score') +
  geom_smooth(method='lm')
```

If you're curious, [here is some discussion](https://groups.google.com/forum/#!topic/ggplot2/1TgH-kG5XMA) on how to also extract and plot the estimated parameters of this fit.

There is also `stat_smooth` and others, and endless ways to specify the complexity, paramaterization, and visualization properties of the model --- we will cover more of this in the next two sessions.


# ggplot Exercises

[(back to top)](#agenda)

**Exercise 1. Multi-`facet`ed.**  Besides changing colors, another easy way to display different groups of plots is using facets, a simple addition to the end of our `ggplot` chain.  For example, using the price by room type example, let's plot each histogram in its own facet:
```{r}
listings %>%
  filter(price < 500) %>%
  ggplot(aes(x=price, y=..density.., fill=room_type)) +
  geom_histogram(binwidth=50, center=25, position='dodge', color='black') +
  labs(x='Price', y='Frac. of Listings', fill='Room type') +
  scale_fill_manual(values=cbbPalette) +
  facet_grid(.~room_type)
```

If we interpret the facet layout as an x-y axis,the `.~room_type` formula means layout nothing (`.`) on the y-axis, against `room_type` on the x-axis.  Sometimes we have too many facets to fit on one line, and we want to let ggplot do the work of wrapping them in a nice way.  For this we can use `facet_wrap()`.  Try plotting the distribution of price, faceted by how many the listing accommodates, using `facet_wrap()`.  Note that now we can't have anything on the y-axis (since we are just wrapping a long line of x-axis facets), so we drop the period from the `~` syntax.

*ANSWER:*
```{r}
listings %>%
  filter(price < 500) %>%
  ggplot(aes(x=price, y=..density..)) +
  geom_histogram(binwidth=50, center=25, position='dodge', color='black') +
  labs(x='Price', y='Frac. of Listings') +
  facet_wrap(~accommodates)
```

Note that if you tried to use the colorblind palette again, you probably ran out of colors and ggplot complained!  (You can use a larger palette, a gradient palette, ...)

**Exercise 2. `geom_tile`**  A useful geometry for displaying heatmaps in `ggplot` is `geom_tile`.  This is typically used when we have data grouped by two different variables, and so we need visualize in 2d.  For example, in an exercise for the last section we looked at median price grouped by # bedrooms and bathrooms.  Try visualizing this table with the `geom_tile` geometry.

*ANSWER:*
```{r}
listings %>%
  group_by(bedrooms, bathrooms) %>%
  summarize(med = median(price)) %>%
  ggplot(aes(x=bedrooms, y=bathrooms, fill=med)) +
  geom_tile()
```

BONUS: We can enforce that the color scale runs between two colors by adjusting a `scale_fill_gradient` theme, like this:
```{r}
listings %>%
  group_by(bedrooms, bathrooms) %>%
  summarize(med = median(price)) %>%
  ggplot(aes(x=bedrooms, y=bathrooms, fill=med)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme_minimal() +
  labs(x='Bedrooms', y='Bathrooms', fill='Median price')
```



**Exercise 3. Getting Dodgy.**  The earlier example where we plotted the histograms grouped by room type is a little hard to read since each set of bars is right next to each other, and maybe we didn't like the `geom_freqpoly` approach.  Another way would be to put a little separation between each threesome of bars.  We can do these kind of tweaks by adjusting the `position=` argument in the histogram geometry.  Instead of just `position='dodge'`, try reproducing the plot using `position=position_dodge(...)`.  (Check out the documentation by typing `?position_dodge` into the console.)

*ANSWER:*
```{r}
listings %>%
  filter(price < 500) %>%
  ggplot(aes(x=price, y=..density.., fill=room_type)) +
  geom_histogram(binwidth=50, center=25, position=position_dodge(width=35), color='black') +
  labs(x='Price', y='Frac. of Listings', fill='Room type') +
  scale_fill_manual(values=cbbPalette)
```

This is still a little awkward having the bars overlap though, and also misleading to have separation between bar when the bins are continous...  check out and try a few of the other position adjustments (for example in the "See Also" of the documentation for position_dodge).

**Exercise 4. Count em up: `geom_count`.**  A common desire in plotting scatters is to have points which occur more often to appear larger (instead of just plotting them on top of each other).  ggplot provides this functionality built-in with `geom_count`.  Try plotting our price vs review score scatter using `geom_count` instead of `geom_point`.  Don't group by score, just plot every listing.  Try also playing around with alpha settings, and fitting a linear regression.

*ANSWER:*
```{r}
listings %>%
  filter(!is.na(review_scores_rating), number_of_reviews > 10) %>%
  ggplot(aes(x=review_scores_rating, y=price)) +
  geom_count(color='blue', alpha=0.2) +
  geom_smooth(method='lm')
```


**(Optional) The Director's Cut.**  Recall we weren't crazy about the way we grouped by a continuous variable in that price vs. review score scatter plot earlier.  We could be a little more precise by doing a histogram simultaneously on price AND score, and plotting the median of each 2d bin.  For more on this, see "Additional Reading" below.  Another way to get around this problem would be to just create a new variable with an approximate, or binned, score rating, like low/medium/high (giving us a factor, instead of a continuous variable) by `cut`ting the continuous variable into bins.

For example, with score, we could create a column `listings$scorecut = cut(listings$review_scores_rating, breaks=3)` that would evenly divide all scores into 3 categories, or cuts.  In our case, we might want to cut the scores by every 5, so we would specify `breaks=seq(0,100,by=5)` which means `breaks=[0,5,10,...,100]`.  Let's try piping that in instead:
```{r}
listings %>%
  filter(!is.na(review_scores_rating)) %>%
  mutate(scorecut = cut(review_scores_rating, breaks=seq(0,100,by=5))) %>%
  group_by(scorecut) %>%
  summarize(medprice = median(price)) %>%
  ggplot(aes(x=scorecut, y=medprice)) +
  geom_point(color='blue')
```

Perfect --- we still see the trend (possibly clearer?) and we are not worried about weird effects from small bins.


# Going Wider and Deeper

[(back to top)](#agenda)

We will now go a little deeper with what tidyr/dplyr and ggplot can do.  We hope to gain an understanding of the intent and philosophy behind the tidy R approach, and in doing, gain a more powerful set of analysis and visualization tools.

## Philosophy

The unifying philosophy of the Tidyverse is:

>- ***Each row is an observation***
>- ***Each column is a variable***
>- ***Each table is an observational unit***

Simple, right?  Yet a lot of data isn't formed that way.  Consider the following table

|Company  | Qtr.1  |  Qtr.2  |  Qtr.3  |  Qtr.4  |
|---------|--------|---------|---------|---------|
|ABC      |$134.01 |$256.77  |$1788.23 |$444.37  |
|XYZ      |$2727.11|$567.23  |$321.01  |$4578.99 |
|GGG      |$34.31  |$459.01  |$123.81  |$5767.01 |

This looks completely acceptable, and is a compact way of representing the information.  However, if we are treating "quarterly earnings" as the observed value, then this format doesn't really follow the tidy philosophy: notice that there are multiple prices (observations) on a row, and there seems to redundancy in the column headers...

In the tidyverse, we'd rather have the table represent "quarterly earning," with each row giving a single observation of a single quarter for a single company, and columns representing the company, quarter, and earning.  Something like this:

|Company  | Quarter |  Earnings  |
|---------|---------|------------|
|ABC      |Qtr.1    |$134.01     |
|ABC      |Qtr.2    |$256.77     |
|ABC      |Qtr.3    |$1788.23    |
|...      |...      |...         |

This is also called the **wide** vs. the **long** format, and we will soon see why it is such a crucial point.

## Changing data between wide and long

Think about our `listings` dataset.  Earlier, we plotted the distribution of daily prices for different room types.  This was easy because this particular slice of the data happened to be tidy: each row was an observation of price for a particular listing, and each column was a single variable, either the room type or the price.

But what if we want to compare the distributions of daily, weekly, and monthly prices?  Now we have a similar situation to the quarterly earnings example from before: now we want each row to have single price, and have one of the columns specify which kind of price we're talking about.

To gather up **wide** data into a **long** format, we can use the `gather` function.  This needs us to specify the desired new columns in standardized form, and the input columns to create those new ones.  First let's make sure our listings data has the converted prices:
```{r}
listings <- listings %>%
  mutate_at(c('price', 'weekly_price', 'monthly_price'),
            funs(as.numeric(gsub('\\$|,', '', .))))
```

Now let's take a look at what `gather` looks like:
```{r}
long.price <- listings %>%
  select(id, name, price, weekly_price, monthly_price) %>%
  gather(freq, tprice, price, weekly_price, monthly_price) %>%
  filter(!is.na(tprice))

long.price %>% head()  # take a peek
```

Cool.  Notice that we left in the unique ID for each listing --- this will help us keep track of how many unique listings we have, since the names are not necessarily unique, and are a little harder to deal with.  Also notice that we filtered for NAs, but we did it *after* the `gather` command.  Otherwise we would remove a whole listing even if it was just missing a weekly or monthly price.

Now this `head` command is only showing us the daily prices but if we don't trust that it worked we can always open it up, or check something like `long.price %>% filter(freq=='weekly_price') %>% head()`

To spread it back out into the original wide format, we can use `spread`
```{r}
long.price %>%
  spread(freq, tprice) %>%
  head()
```


## Visualizing long data

Now what was the point of all that, you may ask?  One reason is to allow us to cleanly map our data to a visualization.  Let's say we want the distributions of daily, weekly, and monthly price, with the color of the line showing which type of price it is.  Before we were able to do this with room type, because each listing had only one room type.  But with price, we would need to do some brute force thing like ... `y1=price, y2=weekly_price, y3=monthly_price`? And `color=` ... ?  This looks like a mess, and it's not valid ggplot commands anyway.

But with the long format data, we can simply specify the color of our line with the `freq` column, which gives which type of observation it is.

```{r}
long.price %>%
  filter(tprice < 1000) %>%
  ggplot(aes(x=tprice, y=..density.., color=freq)) +
  geom_freqpoly(binwidth=50, size=2, center=25) +
  scale_color_manual(values=cbbPalette)
```

There are lots of times we need this little "trick," so you should get comfortable with it --- sometimes it might even be easiest to just chain it in.  Let's plot a bar chart showing the counts of listings with different numbers of bedrooms and bathrooms (we'll filter out half-rooms just to help clean up the plot):
```{r}
listings %>%
  select('Bedrooms'=bedrooms, 'Bathrooms'=bathrooms) %>%
  gather(type, number, Bedrooms, Bathrooms) %>%
  filter(!is.na(number), number %% 1 == 0) %>%
  ggplot(aes(x=number, fill=type)) +
  geom_bar(stat='count', position='dodge', color='black') +
  scale_fill_manual(values=cbbPalette) +
  labs(x='# Rooms', y='# Listings', fill='Room type')
```

Or group by neighborhood and listing type (which will give us a tidy formatted table), get the median daily price, and plot tiles shaded according to that price value using `geom_tile`:
```{r}
listings %>%
  group_by(neighbourhood_cleansed, room_type) %>%
  summarize(med.price = mean(price)) %>%
  ggplot(aes(x=reorder(neighbourhood_cleansed, -med.price), y=room_type, fill=med.price)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  labs(x='', y='', fill='Median price')
```

which shows that "private room" trends and "entire home" trends are not identical: why is Bay Village a high-end entire apartment location but low-end private rooms?  (Small rooms?)  Why are the shared rooms in the South End so much?

Or spread the same data out into a wide format for easy tabular viewing:
```{r}
listings %>%
  group_by(neighbourhood_cleansed, room_type) %>%
  summarize(med.price = mean(price)) %>%
  filter(neighbourhood_cleansed %in% c('Beacon Hill', 'Downtown', 'Fenway',
                                       'Back Bay', 'West End')) %>%
  spread(room_type, med.price)
```


## Joining datasets

Our last topic will be how to **join** two data frames together.  We'll introduce the concept with two toy data frames, then apply it to our AirBnB data.

### Join together, right now, over me...

(The following example adapted from [here](https://rpubs.com/bradleyboehmke/data_wrangling).)  Let's say `table1` is
```{r}
table1 = data.frame(name=c('Paul', 'John', 'George', 'Ringo'),
                    instrument=c('Bass', 'Guitar', 'Guitar', 'Drums'),
                    stringsAsFactors=F)
table1  # take a look
```

and `table2` is
```{r}
table2 = data.frame(name=c('John', 'George', 'Jimi', 'Ringo', 'Sting'),
                    member=c('yes', 'yes', 'no', 'yes', 'no'),
                    stringsAsFactors=F)
table2
```

then we might want to join these datasets so that we have a `name`, `instrument`, and `member` column, and the correct information filled in from both datasets (with NAs wherever we're missing the info).  This operation is called a `full_join` and would give us this:

```{r}
full_join(table1, table2, by='name')
```

Notice we have to specify a **key** column, which is what column to join `by`, in this case `name`.

We might also want to make sure we keep all the rows from the first table (the "left" table) but only add rows from the second ("right") table if they match existing ones from the first.  This called a `left_join` and gives us
```{r}
left_join(table1, table2, by='name')
```

since "Jimi" and "Sting" don't appear in the `name` column of `table1`.

Left and full joins are both called "outer joins" (you might think of merging two circles of a Venn diagram, and keeping all the non-intersecting "outer" parts).  However, we might want to use only rows whose key values occur in both tables (the intersecting "inner" parts) --- this is called an `inner_join` and gives us
```{r}
inner_join(table1, table2, by='name')
```

There is also `semi_join`, `anti_join`, ways to handle coercion, ways to handle different column names ... we don't have time to cover all the variations here, but let's try using some basic concepts on our AirBnB data.

### Applying joins

Let's say we have a tidy table of the number of bathrooms and bedrooms for each listing, which we get by doing
```{r}
rooms <- listings %>%
  select(name, bathrooms, bedrooms) %>%
  gather(room.type, number, bathrooms, bedrooms)
```

But we may also want to look at the distribution of daily prices, which we can store as
```{r}
prices <- listings %>%
  select(name, price) %>%
  mutate(price = as.numeric(gsub('\\$|,', '', price)))
```

Now, we can do a full join to add a `price` column.
```{r}
rooms.prices <- full_join(rooms, prices, by='name')
```

This gives us a table with the number of bed/bathrooms separated out in a tidy format (so it is amenable to ggplot), but also prices tacked on each row (so we can incorporate that into the visualization).  Let's try a boxplot of price, by number of rooms, and use facets to separate out the two different types of room.  (We will also filter out half-rooms just to help clean up the plot.)
```{r}
rooms.prices %>%
  filter(!is.na(number), number %% 1 == 0) %>%
  mutate(number = as.factor(number)) %>%
  ggplot(aes(x=number, y=price, fill=room.type)) +
  geom_boxplot() +
  facet_grid(~room.type) +
  labs(x='# of Rooms', y='Daily price', fill='Room type')
```

This allows us to easily use the `room.type` column (created in the gather before) to set our fill color and facet layout, but still have access to all the price information from the original dataset.  This visualization shows us that there is a trend of increasing price with increasing number of bathrooms and bedrooms, but it is not a strict one, and seems to taper off at around 2 bedrooms for example.

In the next sessions, we will need data from the `listings.csv` file and the other datasets `calendar.csv` and `reviews.csv`, so we will use these joins again.

# Bonus: Visualizing Map Data

Spatial data has become much easier to work with in R in recent years, thanks to some new packages. Here, we'll work with two of the most powerful:

- `sf`: Tidyverse-compatible tools for loading and manipulating spatial data.
- `leaflet`: Easy interactive maps via the Javascript library Leaflet.

Let's say we want to plot average listing prices by zip code.

First, let's grab some zip code shapes from [Analyze Boston](http://bostonopendata-boston.opendata.arcgis.com/datasets/53ea466a189b4f43b3dfb7b38fa7f3b6_1). Unzip this in your working directory and load it into a dataframe:

```{r}
shp <- read_sf('shape/ZIP_Codes.shp')
```

Here, `shp` is an sf ("simple features") dataframe. You can manipulate it using the usual dplyr operations; it just has a column containing a special "geometry" column that has the geometry (point, line, polygon, etc.) corresponding to each row.

Next, let's grab the relevant information from the listings data and turn it into dataframe:

```{r}
listing_geos <- listings %>%
  select(price, longitude, latitude) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = st_crs(shp))
```

Note: the `crs` argument sets the coordinate system of our new spatial dataframe. Since we're going to join it with `shp`, we want their coordinate systems to be the same.

Let's join these two and aggregate the dataframes.

A couple notes:
 - The `join` argument tells `st_join` how to match shapes. In this case, we join a shape from `shp` with a point in `listing_geos` if the shape contains the point.
 - Setting the `do_union` argument to false in `summarise` speeds up the operation a bit, telling `summarise` that each group only has a single geometry.

```{r}
by.zip <- st_join(shp, listing_geos, join = st_contains) %>%
  group_by(ZIP5) %>%
  summarise(price = mean(price, na.rm = TRUE),
            listings = n(),
            do_union = FALSE)
```

Finally, we plot the map with leaflet:

```{r}
leaflet(by.zip) %>%
  addProviderTiles("CartoDB.Positron") %>%
  addPolygons(weight = 1,
              fillColor = ~colorNumeric("YlOrRd", domain = price)(price),
              popup = ~as.character(price))
```

Breaking this down:
- `leaflet(by.zip)` creates a leaflet map object and tells it to refer to `by.zip` for data.
- `addProviderTiles` draws the map tiles.
- `addPolygons` draws the shapes, with some "aesthetics" just like ggplot: weight, fillColor, and popup. The main difference is that we specify them as formulas (denoted by the `~` character) instead of just variable names.

# Wrapping Up

In this session, we introduced some basics of data wrangling and visualization in R.  Specifically, we showed some basic operations using out-of-the-box R commands, introduced the powerful framework of the "Tidyverse," its accompanying visualization suite `ggplot`, discussed some of the elegant data philosophy behind these libraries, briefly covered some more involved operations like gather/spread and dataset joins, and hinted at deeper applications such as predictive analytics and time series analysis that we will cover in the next two sessions.

## Further reading

Some of the infinitude of subjects we did not cover are: heatmaps and 2D histograms, statistical functions, plot insets, ...  And even within the Tidyverse, don't feel you need to limit yourself to `ggplot`.  Here's a good overview of some [2d histogram techniques](http://www.everydayanalytics.ca/2014/09/5-ways-to-do-2d-histograms-in-r.html), a discussion on [overlaying a normal curve over a histogram](http://stackoverflow.com/questions/5688082/ggplot2-overlay-histogram-with-density-curve), a workaround to fit multiple plots in [one giant chart](http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/).  In general, if you can dream it up, someone else has too and there's a solution/explanation on the internet.

The best way to learn R, and data analysis in general, is not to read blogs and papers, but to *get out there and do it*.  There are always intriguing competitions on data hosting websites like [Kaggle](http://www.kaggle.com), and there many areas like [sports analytics](http://www.footballoutsiders.com), [political forecasting](http://www.electoral-vote.com/evp2016/Info/data.html), [historical analysis](https://t.co/3WCaDxGnJR), and countless others that have [clean](http://http://www.pro-football-reference.com/), [open](http://www.kdnuggets.com/datasets/index.html), and interesting data just waiting for you to `read.csv`.  You don't need proprietary data [to make headlines](http://fivethirtyeight.com/features/a-plagiarism-scandal-is-unfolding-in-the-crossword-world/), and some data that seems like it would be hard to get is actually [out there in the wild](https://www.kaggle.com/kaggle/hillary-clinton-emails).

These are hobbyist applications, but we also hope this session has sparked your interest in applying analytics to your own research.  Nearly every modern research field has been touched by the power and pervasiveness of data, and having the tools to take advantage of this in your field is a skill with increasing value.

And plus, it's pretty fun.
