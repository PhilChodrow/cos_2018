{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver Callbacks in JuMP\n",
    "\n",
    "MIP solvers are complicated combinations of many techniques: cutting planes, heuristics, branching rules, etc.\n",
    "\n",
    "Some solvers allow you to customize aspects of the solve process in a deeper way than just setting options for these parameters. You can provide code to be run when certain events happen, and the solver **calls back** to these functions to ask what action(s) should be taken. Why might you want to do this? Some situations:\n",
    "\n",
    "* **Informational Callbacks:** you might want to extract the intermediate solutions found in the branch&bound tree, or other useful information during the solution process, and not work only with the optimal solution.\n",
    "\n",
    "* **Lazy Constraints:** you might not want to enumerate all of your constraints at the onset, and enforce certain constraints only when they're needed.\n",
    "\n",
    "We'll explore these cases in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up: Linear Regression and its variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with linear regression: suppose we have a data matrix $A$, and vector $b$, and we want to estimate a vector $x$ such that:\n",
    "\n",
    "$$\n",
    "\\underset{x}{\\min}\\ || Ax-b ||_2^2\n",
    "$$\n",
    "\n",
    "One way of expressing it in JuMP is through the following formulation:\n",
    "\n",
    "$$\n",
    "\\underset{x}{\\min}\\ \\sum_i z_i^2 \\\\\n",
    "\\text{s.t.}\\quad z_i = a_i^\\top x - b_i \\quad\\forall i\n",
    "$$\n",
    "\n",
    "where $a_i^\\top$ is the $i$-th row of the data matrix $A$.\n",
    "\n",
    "Let's first create the data $A$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srand(123)\n",
    "\n",
    "n = 20\n",
    "p = 5\n",
    "\n",
    "real_x = 10*rand(p)\n",
    "A = rand(n,p)\n",
    "b = A*real_x + rand(n);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's formulate the problem in JuMP and solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Gurobi\n",
    "\n",
    "m = Model(solver=GurobiSolver())\n",
    "@variable(m, x[1:p])\n",
    "@variable(m, z[1:n])\n",
    "\n",
    "@objective(m, Min, sum(z[i]^2 for i in 1:n))\n",
    "\n",
    "@constraint(m, [i=1:n], z[i] == sum(A[i,j]*x[j] for j in 1:p) - b[i])\n",
    "        \n",
    "solve(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the value of $x$ we found, and compare it to its true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show getvalue(x);\n",
    "@show real_x;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: When might you want to do this yourself (in JuMP/etc), versus using a commercial/opensource package?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**\\[Exercise\\]**: Non-negative Least Squares\n",
    "\n",
    "> How might we modify the formulation above to solve for non-negative least squares, i.e. $\\underset{x\\geq 0}{\\min}\\ || Ax-b ||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**\\[Exercise\\]**: Sparse Linear Regression\n",
    "\n",
    "> How might we modify the formulation above if we know at most $k$ of the coefficients are non-zero? i.e. $\\underset{x: ||x||_0\\leq k}{\\min}\\ || Ax-b ||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: Do you know of other ways of getting sparse (as in low number of non-zero coefficients) solutions in a linear regression setting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Extracting Intermediate Solutions\n",
    "Rather than having to parse the solver log, sometimes querying for information (amongst other things) might be useful for you to do convergence plots (or perform other diagnostics). Informational callbacks are added to a JuMP model with the `addinfocallback(m::Model, f::Function; when::Symbol)` function, where the `when` argument should be one of `:MIPNode`, `:MIPSol` or `:Intermediate` (listed under `cbgetstate()` in the [MathProgBase documentation](https://mathprogbasejl.readthedocs.io/en/latest/callbacks.html)).\n",
    "\n",
    "Suppose we wish to extract all the incumbent solutions generated during the branch&bound process. Here's how we can modify the code above to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Gurobi\n",
    "\n",
    "M = 10000\n",
    "k = 2\n",
    "\n",
    "m = Model(solver=GurobiSolver())\n",
    "@variable(m, x[1:p] >= 0)\n",
    "@variable(m, y[1:p], Bin)\n",
    "@variable(m, z[1:n])\n",
    "\n",
    "@objective(m, Min, sum(z[i]^2 for i in 1:n))\n",
    "\n",
    "@constraint(m, [i=1:n], z[i] == sum(A[i,j]*x[j] for j in 1:p) - b[i])\n",
    "@constraint(m,[j=1:p], x[j] <= M*y[j])\n",
    "@constraint(m,[j=1:p], x[j] >= -M*y[j])\n",
    "@constraint(m, sum(y[j] for j in 1:p) <= k)\n",
    "\n",
    "### --- NEW ---\n",
    "\n",
    "### END OF NEW\n",
    "\n",
    "solve(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the incumbent solutions generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as their corresponding objective values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[norm(A*sol_x-b)^2 for sol_x in solutionvalues]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: Can you identify the objective values of the incumbent solution in the solver log above? Which of the incumbent solutions were found via heuristics, and which ones were found via branch&bound?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**\\[Exercise\\]**: Information Callbacks\n",
    "\n",
    "> How might you organize the code such that you can extract\n",
    "\n",
    "> - the objective function value of the current solution\n",
    "> - the current best bound on the optimal solution\n",
    "> - the current time taken since the start of the solution process\n",
    "\n",
    "> (Hint: http://www.juliaopt.org/JuMP.jl/0.15/callbacks.html#informational-callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2- Modeling using Lazy Constraints\n",
    "\n",
    "### Motivation\n",
    "**Question**: Now suppose we wish to solve the sparse linear regression problem\n",
    "\n",
    "$$\n",
    "\\underset{x}{\\min}\\ \\sum_i z_i^2 \\\\\n",
    "\\text{s.t.}\\quad z_i = a_i^\\top x - b \\quad\\forall i \\\\\n",
    "x_j \\leq My_j \\\\\n",
    "x_j \\geq -My_j \\\\\n",
    "\\sum_{j=1}^p y_j \\leq k \\\\\n",
    "y_j\\in\\{0,1\\}\\quad\\forall j=1,\\dots,p\n",
    "$$\n",
    "\n",
    "but as a sequence of mixed integer linear programs, rather than a single mixed integer quadratic program. How might we go about doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: ![](outerapprox.jpg)\n",
    "(image source: http://www.mit.edu/~dimitrib/Polyhedral_Approx_Tsinghua.pdf)\n",
    "\n",
    "In general, we might want to perform a \"linearization\" of some (often convex) function, where we perform a succession of \"outer-approximations\" that converges towards the underlying function we wish to optimize over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: For readers who are interested in cutting-plane methods, the following might be a good reference: https://web.stanford.edu/class/ee392o/localization-methods.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: What are potential issues we might run into, if we wish to implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "Lazy constraints are useful when **the full set of constraints is too large to explicitly include in the initial formulation**. We might want to modify the MIP solution process such that when we have a new solution (for example with a heuristic or by solving a problem at a node in the branch-and-bound tree), we are given the opportunity to generate additional constraint(s). This allows us to generate them only upon demand, and stop the process based on our termination criteria.\n",
    "\n",
    "There are three important steps to providing a lazy constraint callback:\n",
    "\n",
    "1. we must write a function that will analyze the current solution that takes a single argument, e.g. `mylazycongenerator(cb)`, where cb is a reference to the callback management code inside JuMP.\n",
    "2. do whatever analysis of the solution you need to inside your function to generate the new constraint before adding it to the model with `@lazyconstraint(cb, myconstraint)` (instead of the usual `@constraint(m, myconstraint)`).\n",
    "3. finally we notify JuMP that this function should be used for lazy constraint generation using the `addlazycallback(m, mylazycongenerator)` function before we call `solve(m)`.\n",
    "\n",
    "For more information, see http://www.juliaopt.org/JuMP.jl/0.15/callbacks.html#lazy-constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Now, we'd like to implementing sparse linear regression using lazy constraints, instead of formulating it as a quadratic optimization problem. To do so, we first represent the objective function\n",
    "\n",
    "$$\n",
    "f(z) = ||z||_2^2\n",
    "$$\n",
    "\n",
    "as the pointwise maximum of affine functions:\n",
    "\n",
    "$$\n",
    "f(z) = \\underset{\\beta}{\\sup} ||\\beta||_2^2 + 2\\beta^\\top(z-\\beta)\n",
    "$$\n",
    "\n",
    "(or equivalently)\n",
    "\n",
    "$$\n",
    "f(z) = \\min\\ \\eta \\\\\n",
    "\\text{s.t.}\\ \\ \\eta \\geq ||\\beta||_2^2 + 2\\beta^\\top(z-\\beta) \\quad\\forall\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding it up\n",
    "So our JuMP model becomes:\n",
    "\n",
    "$$\n",
    "\\underset{x}{\\min}\\ \\eta \\\\\n",
    "\\text{s.t.}\\quad \n",
    "\\eta \\geq ||\\beta||_2^2 + 2\\beta^\\top(z-\\beta) \\quad\\forall \\beta \\\\\n",
    "z_i = a_i^\\top x - b \\quad\\forall i \\\\\n",
    "x_j \\leq My_j \\\\\n",
    "x_j \\geq -My_j \\\\\n",
    "\\sum_{j=1}^p y_j \\leq k \\\\\n",
    "y_j\\in\\{0,1\\}\\quad\\forall j=1,\\dots,p\n",
    "$$\n",
    "\n",
    "where we represent the set of (infinite) constraints:\n",
    "\n",
    "$$\n",
    "\\eta \\geq ||\\beta||_2^2 + 2\\beta^\\top(z-\\beta) \\quad\\forall \\beta\n",
    "$$\n",
    "\n",
    "using lazy constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**\\[Exercise\\]**: Lazy Callbacks\n",
    "\n",
    "> Implement the above formulation in JuMP (reference: http://www.juliaopt.org/JuMP.jl/0.15/callbacks.html#lazy-constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Gurobi\n",
    "\n",
    "M = 10000\n",
    "k = 2\n",
    "\n",
    "m = Model(solver=GurobiSolver())\n",
    "@variable(m, x[1:p] >= 0)\n",
    "@variable(m, y[1:p], Bin)\n",
    "@variable(m, z[1:n])\n",
    "@variable(m, eta >= 0)\n",
    "\n",
    "@objective(m, Min, eta)\n",
    "\n",
    "@constraint(m, [i=1:n], z[i] == sum(A[i,j]*x[j] for j in 1:p) - b[i])\n",
    "@constraint(m,[j=1:p], x[j] <= M*y[j])\n",
    "@constraint(m,[j=1:p], x[j] >= -M*y[j])\n",
    "@constraint(m, sum(y[j] for j in 1:p) <= k)\n",
    "\n",
    "### --- Your solution here ---\n",
    "function lazyleastsqs(cb)\n",
    "end\n",
    "addlazycallback(m, lazyleastsqs)\n",
    "### --- End of solution ---\n",
    "\n",
    "solve(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: So far, we've been looking at modelling a convex function as the pointwise maximum of a (possibly infinite) set of affine functions in the objective function. Can you anticipate other optimization problems that might be usefully modelled using a large (possibly infinite) number of linear constraints?\n",
    "\n",
    "See for e.g. Robust Portfolio Optimization and Travelling Salesman by Iain in [last year's IAP class](https://github.com/joehuchette/OR-software-tools-2015/blob/master/7-adv-optimization/Callbacks.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: Sometimes a good polyhedral outer approximation might need too many linear constraints, and might benefit from \"extended formulations\". See http://www.mit.edu/~mlubin/micp-cribb.pdf (from slides 19 onwards) for some details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Application - the Traveling Salesman Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A canonical example of a problem where a lazy constraint formulation comes in useful is the **Traveling Salesman Problem (TSP)**.\n",
    "> Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits every city exactly once?\n",
    "\n",
    "Let's create an example where cities are random points in the 2D plane, and the distance function is just the Euclidean metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gadfly, GraphPlot, LightGraphs\n",
    "using JuMP, Gurobi\n",
    "N = 50\n",
    "srand(1)\n",
    "# create random points in the plane\n",
    "locations = [[rand(),rand()] for i=1:N]\n",
    "locs_x = [locations[i][1] for i=eachindex(locations)]\n",
    "locs_y = [locations[i][2] for i=eachindex(locations)]\n",
    "# distance matrix\n",
    "dists = zeros(length(locations), length(locations))\n",
    "for (i, location1) in enumerate(locations), (j, location2) in enumerate(locations)\n",
    "    dists[i,j] = norm(location2 - location1)\n",
    "end\n",
    "# show the locations\n",
    "g = Graph(N)\n",
    "gplot(g, locs_x, locs_y, nodelabel=collect(1:N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "\n",
    "Let's try to formulate the traveling salesman problem as an integer program.\n",
    "Let the cities be indexed from 1 to N.\n",
    "Let $d_{ij}$ be the distance between city $i$ and city $j$.\n",
    "\n",
    "Decision variables: $x_{ij}=\\begin{cases} 1,\\quad \\text{if city $i$ and city $j$ are adjacent in the shortest tour}\\\\\n",
    "0,\\quad \\text{otherwise.}\\end{cases}$\n",
    "\n",
    "N.B. $x_{ij}$ and $x_{ji}$ are redundant ($x_{ij}=x_{ji}$), so we only define the variable $x_{ij}$ for $i < j$. Then we can formulate the following integer program.\n",
    "\n",
    "$$\n",
    "\\underset{x}{\\min}\\ \\sum_{i=1}^N\\sum_{j=i+1}^N d_{ij}x_{ij} \\\\\n",
    "\\text{s.t.}\\quad \n",
    "\\sum_{j=i+1}^N x_{ij} + \\sum_{j=1}^{i-1}x_{ji} = 2 \\quad\\forall i, 1\\le i \\le N \\\\\n",
    "x_{ij}\\in\\{0,1\\}\\quad\\forall i,j \\text{ s.t. } 1\\le i < j \\le N\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsp = Model(solver=GurobiSolver())\n",
    "\n",
    "# Create the binary variables\n",
    "\n",
    "# Add the constraints that every city needs to be visited exactly once\n",
    "\n",
    "# Define the objective to get the shortest distance\n",
    "\n",
    "solve(tsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the solution we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot the solution\n",
    "function plotTSP(locs_x, locs_y, xVal)\n",
    "    n = length(locs_x)\n",
    "    g = Graph(n)\n",
    "    for i=1:n, j=(i+1):n\n",
    "        if xVal[i,j] > 0.5\n",
    "            add_edge!(g, i, j)\n",
    "        end\n",
    "    end\n",
    "    gplot(g, locs_x, locs_y, nodelabel=collect(1:n))\n",
    "end\n",
    "plotTSP(locs_x, locs_y, getvalue(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! Our formulation is missing something! What are some potential ways to fix it?\n",
    "\n",
    "One common way is **subtour elimination** constraints, to prevent the final solution from having any small cycles, i.e. cycles that do not include all the nodes.\n",
    "\n",
    "Given a subtour $S\\subset \\{1,\\ldots,N\\}$, a subtour elimination constraint looks like:\n",
    "$$\\sum_{i\\in S} \\left(\\sum_{j\\notin S, j > i}x_{ij}+\\sum_{j\\notin S, j < i}x_{ji}\\right) \\ge 2$$.\n",
    "\n",
    "> **[Exercise]** How many subtours are there for $N=4$? for $N=5$? for general $N$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $N$ grows larger, the number of subtour elimination constraints grows exponentially. It is therefore impractical to add all of these constraints into the model.\n",
    "\n",
    "Instead, we generate these constraints lazily. Every time Gurobi has an incumbent solution, we find the shortest subtour in the solution, and add a lazy constraint eliminating this particular subtour.\n",
    "\n",
    "The function below finds the shortest cycle in a given solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function subtour(xVal, nNodes, criterion=\"shortest\")\n",
    "    graph = Graph(nNodes)\n",
    "    for i=1:nNodes, j=(i+1):nNodes\n",
    "        if xVal[i,j] > 0.5\n",
    "            add_edge!(graph, i, j)\n",
    "        end\n",
    "    end\n",
    "    visited = falses(nv(graph))\n",
    "    cycles = []\n",
    "    selected = [[] for i=vertices(graph)]\n",
    "    for e = edges(graph)\n",
    "        push!(selected[src(e)], dst(e))\n",
    "        push!(selected[dst(e)], src(e))\n",
    "    end\n",
    "    while true\n",
    "        current = findfirst(visited, false)\n",
    "        thiscycle = [current]\n",
    "        while true\n",
    "            visited[current] = true\n",
    "            neighbors = [x for x in selected[current] if !visited[x]]\n",
    "            if isempty(neighbors)\n",
    "                break\n",
    "            end\n",
    "            current = neighbors[1]\n",
    "            push!(thiscycle, current)\n",
    "        end\n",
    "        push!(cycles, thiscycle)\n",
    "        if sum(length(cycle) for cycle in cycles) == nv(graph)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    if criterion == \"shortest\"\n",
    "        selectedCycle = cycles[indmin([length(cycle) for cycle in cycles])]\n",
    "    elseif criterion == \"longest\"\n",
    "        selectedCycle = cycles[indmax([length(cycle) for cycle in cycles])]\n",
    "    else\n",
    "        selectedCycle = rand(cycles)\n",
    "    end\n",
    "    return selectedCycle, length(selectedCycle)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **[Exercise]** Given the method `subtour` above, let's write a callback to eliminate subtours lazily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "tsp_cb = Model(solver=GurobiSolver(LazyConstraints=1))\n",
    "\n",
    "# Define variables\n",
    "@variable(tsp_cb, x[i=1:N, j=(i+1):N], Bin)\n",
    "\n",
    "# Every city must be visited at least once\n",
    "@constraint(tsp_cb, visitEveryCity[i=1:N], sum(x[i,j] for j=(i+1):N) + sum(x[j,i] for j=1:(i-1)) == 2)\n",
    "\n",
    "# Define objective to be optimized\n",
    "@objective(tsp_cb, Min, sum(dists[i,j] * x[i,j] for i=1:N, j=(i+1):N))\n",
    "\n",
    "function subTourElimination(cb)\n",
    "    cycle, length = subtour(getvalue(x), N, \"shortest\")\n",
    "    if length < N\n",
    "        # your job - insert lazy constraint here\n",
    "        \n",
    "    else\n",
    "        # no more subtours need to be eliminated\n",
    "    end\n",
    "end\n",
    "                \n",
    "addlazycallback(tsp_cb, subTourElimination)\n",
    "\n",
    "solve(tsp_cb)\n",
    "plotTSP(locs_x, locs_y, getvalue(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **[Discussion/Exercise]** Choice of subtour.\n",
    "\n",
    "> a) At each incumbent solution, we only add a constraint removing one subtour, instead of all subtours. What do you think is the rationale behind this choice?\n",
    "\n",
    "> b) The provided `subtour` method has a third argument, which lets you select which subtour is eliminated (options are \"shortest\", \"longest\", and \"random\"). Compare the effect of these three choices of subtour on the solver performance, in particular the number of incumbent solutions explored by Gurobi (HINT: how can you get this information?), and any other metrics you feel could be relevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
